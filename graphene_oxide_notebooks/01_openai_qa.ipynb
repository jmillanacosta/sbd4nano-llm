{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nest-asyncio in /home/javier/miniconda3/lib/python3.9/site-packages (1.5.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain\n",
    "%pip install -qU huggingface_hub\n",
    "%pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Necessary to import OpenAI\n",
    "with open('assets/openai_api_key', 'r') as f:\n",
    "    openai_api_key = f.read()\n",
    "with open('assets/huggingface_api_key', 'r') as f:\n",
    "    huggingface_api_key = f.read()\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ['HUGGINGFACEHUB_API_KEY'] = huggingface_api_key\n",
    "import pandas as pd\n",
    "import paperqa\n",
    "from langchain.llms import OpenAI\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI QA using paper-qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = '../data/docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cf60dcf1c3a14a15ac9ac3502616abf in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cf60dcf1c3a14a15ac9ac3502616abf in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cf60dcf1c3a14a15ac9ac3502616abf in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Mon, 26 Jun 2023 15:50:44 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'wut-1', 'openai-processing-ms': '30287', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '8cf60dcf1c3a14a15ac9ac3502616abf', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7dd69aca7e361c7e-AMS', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.chat_models.openai.acompletion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/javier/sbd4nano-llm/graphene_oxide_notebooks/01_openai_qa.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/javier/sbd4nano-llm/graphene_oxide_notebooks/01_openai_qa.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m my_docs:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/javier/sbd4nano-llm/graphene_oxide_notebooks/01_openai_qa.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     docs\u001b[39m.\u001b[39madd(d)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/javier/sbd4nano-llm/graphene_oxide_notebooks/01_openai_qa.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m answer1 \u001b[39m=\u001b[39m docs\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mWhich characteristics of Graphene Oxide correlate with its ability to inflict oxidative stress on the cell?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/javier/sbd4nano-llm/graphene_oxide_notebooks/01_openai_qa.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(answer1\u001b[39m.\u001b[39mformatted_answer)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/paperqa/docs.py:488\u001b[0m, in \u001b[0;36mDocs.query\u001b[0;34m(self, query, k, max_sources, length_prompt, marginal_relevance, answer, key_filter, get_callbacks)\u001b[0m\n\u001b[1;32m    486\u001b[0m     loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mnew_event_loop()\n\u001b[1;32m    487\u001b[0m     asyncio\u001b[39m.\u001b[39mset_event_loop(loop)\n\u001b[0;32m--> 488\u001b[0m \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maquery(\n\u001b[1;32m    490\u001b[0m         query,\n\u001b[1;32m    491\u001b[0m         k\u001b[39m=\u001b[39;49mk,\n\u001b[1;32m    492\u001b[0m         max_sources\u001b[39m=\u001b[39;49mmax_sources,\n\u001b[1;32m    493\u001b[0m         length_prompt\u001b[39m=\u001b[39;49mlength_prompt,\n\u001b[1;32m    494\u001b[0m         marginal_relevance\u001b[39m=\u001b[39;49mmarginal_relevance,\n\u001b[1;32m    495\u001b[0m         answer\u001b[39m=\u001b[39;49manswer,\n\u001b[1;32m    496\u001b[0m         key_filter\u001b[39m=\u001b[39;49mkey_filter,\n\u001b[1;32m    497\u001b[0m         get_callbacks\u001b[39m=\u001b[39;49mget_callbacks,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    499\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nest_asyncio.py:83\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     81\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     85\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/nest_asyncio.py:106\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     heappop(scheduled)\n\u001b[1;32m    101\u001b[0m timeout \u001b[39m=\u001b[39m (\n\u001b[1;32m    102\u001b[0m     \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m ready \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping\n\u001b[1;32m    103\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mmax\u001b[39m(\n\u001b[1;32m    104\u001b[0m         scheduled[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_when \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime(), \u001b[39m0\u001b[39m), \u001b[39m86400\u001b[39m) \u001b[39mif\u001b[39;00m scheduled\n\u001b[1;32m    105\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 106\u001b[0m event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    109\u001b[0m end_time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime() \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/selectors.py:469\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout, max_ev)\n\u001b[1;32m    470\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get a list of paths\n",
    "my_docs = [doc_path + file for file in list(os.listdir(doc_path))]\n",
    "\n",
    "docs = paperqa.Docs()\n",
    "for d in my_docs:\n",
    "    docs.add(d)\n",
    "\n",
    "answer1 = docs.query(\"Which characteristics of Graphene Oxide correlate with its ability to inflict oxidative stress on the cell?\")\n",
    "print(answer1.formatted_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
